{
  "examples": [
    {
      "id": "netlify-deploy",
      "title": "Deploy to Netlify",
      "description": "Python script to automate deployment to Netlify",
      "language": "python",
      "icon": "üöÄ",
      "tags": ["deployment", "automation", "netlify"],
      "code": "import os\nimport requests\nimport json\nimport time\nimport zipfile\nimport tempfile\nimport shutil\n\n# Netlify API credentials\nNETLIFY_API_TOKEN = \"nfp_y18gz9tbir8366\"\nSITE_NAME = \"florist-elegant-landing\"\n\n# Directory to deploy\nDIRECTORY_PATH = \"/workspace/cloudflare\"\n\ndef create_zip_archive(directory_path):\n    \"\"\"Create a zip archive of the directory to deploy.\"\"\"\n    temp_dir = tempfile.mkdtemp()\n    zip_path = os.path.join(temp_dir, \"deploy.zip\")\n    \n    with zipfile.ZipFile(zip_path, 'w', zipfile.ZIP_DEFLATED) as zipf:\n        for root, dirs, files in os.walk(directory_path):\n            for file in files:\n                file_path = os.path.join(root, file)\n                arcname = os.path.relpath(file_path, directory_path)\n                zipf.write(file_path, arcname)\n    \n    return zip_path, temp_dir\n\ndef deploy_to_netlify(zip_path):\n    \"\"\"Deploy the zip archive to Netlify.\"\"\"\n    # Get site ID using the site name\n    sites_url = \"https://api.netlify.com/api/v1/sites\"\n    headers = {\n        \"Authorization\": f\"Bearer {NETLIFY_API_TOKEN}\"\n    }\n    \n    response = requests.get(sites_url, headers=headers)\n    sites = response.json()\n    \n    site_id = None\n    for site in sites:\n        if site[\"name\"] == SITE_NAME:\n            site_id = site[\"id\"]\n            break\n    \n    if not site_id:\n        print(f\"Site '{SITE_NAME}' not found\")\n        return False\n    \n    # Deploy to the site\n    deploy_url = f\"https://api.netlify.com/api/v1/sites/{site_id}/deploys\"\n    \n    with open(zip_path, 'rb') as zip_file:\n        files = {'file': zip_file}\n        response = requests.post(deploy_url, headers=headers, files=files)\n    \n    if response.status_code == 200:\n        deploy_data = response.json()\n        print(f\"Deployment successful. Deployment URL: {deploy_data['deploy_url']}\")\n        return True\n    else:\n        print(f\"Deployment failed with status code {response.status_code}\")\n        print(response.text)\n        return False\n\ndef main():\n    print(f\"Creating ZIP archive of {DIRECTORY_PATH}...\")\n    zip_path, temp_dir = create_zip_archive(DIRECTORY_PATH)\n    \n    try:\n        print(\"Deploying to Netlify...\")\n        deploy_to_netlify(zip_path)\n    finally:\n        # Clean up\n        shutil.rmtree(temp_dir)\n        print(\"Temporary files removed\")\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "id": "data-visualization",
      "title": "Interactive Data Visualization",
      "description": "Create interactive charts with Python and Plotly",
      "language": "python",
      "icon": "üìä",
      "tags": ["visualization", "data science", "plotly"],
      "code": "import pandas as pd\nimport plotly.express as px\nimport plotly.io as pio\n\n# Sample data creation\ndef generate_sample_data():\n    \"\"\"Generate sample sales data for visualization.\"\"\"\n    data = {\n        'Month': ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n                  'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'],\n        'Sales': [4200, 4800, 5100, 5400, 6200, 7100, \n                  8500, 9200, 8100, 7200, 6500, 7800],\n        'Expenses': [3800, 4100, 4300, 4500, 5100, 5700, \n                     6200, 6800, 6300, 5900, 5200, 6100],\n        'Profit': [400, 700, 800, 900, 1100, 1400, \n                   2300, 2400, 1800, 1300, 1300, 1700]\n    }\n    return pd.DataFrame(data)\n\ndef create_visualization():\n    \"\"\"Create interactive data visualizations using Plotly.\"\"\"\n    # Generate data\n    df = generate_sample_data()\n    \n    # Create a line chart for sales and expenses\n    fig1 = px.line(df, x='Month', y=['Sales', 'Expenses', 'Profit'], \n                   title='Monthly Financial Performance',\n                   template='plotly_white')\n    \n    # Customize layout\n    fig1.update_layout(\n        xaxis_title='Month',\n        yaxis_title='Amount ($)',\n        legend_title='Metric',\n        hovermode='x unified'\n    )\n    \n    # Create a bar chart for profit\n    fig2 = px.bar(df, x='Month', y='Profit', \n                  title='Monthly Profit',\n                  template='plotly_white',\n                  color='Profit',\n                  color_continuous_scale=px.colors.sequential.Viridis)\n    \n    # Customize layout\n    fig2.update_layout(\n        xaxis_title='Month',\n        yaxis_title='Profit ($)',\n        coloraxis_showscale=False\n    )\n    \n    # Save the figures as HTML files\n    pio.write_html(fig1, 'financial_performance.html', auto_open=True)\n    pio.write_html(fig2, 'monthly_profit.html', auto_open=True)\n    \n    print(\"Visualizations created and saved as HTML files\")\n\nif __name__ == \"__main__\":\n    create_visualization()"
    },
    {
      "id": "web-scraper",
      "title": "Web Scraper with BeautifulSoup",
      "description": "Python script to scrape product information from e-commerce sites",
      "language": "python",
      "icon": "üîç",
      "tags": ["web scraping", "data extraction", "beautifulsoup"],
      "code": "import requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport time\nimport random\nfrom urllib.parse import urljoin\n\n# User agent to mimic a browser\nUSER_AGENT = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n\n# Headers for the request\nHEADERS = {\n    \"User-Agent\": USER_AGENT,\n    \"Accept-Language\": \"en-US,en;q=0.5\",\n}\n\ndef scrape_product_listings(url, max_pages=3):\n    \"\"\"Scrape product listings from an e-commerce site.\"\"\"\n    all_products = []\n    current_page = 1\n    \n    while current_page <= max_pages:\n        # Construct the URL for pagination\n        if current_page > 1:\n            paginated_url = f\"{url}?page={current_page}\"\n        else:\n            paginated_url = url\n        \n        print(f\"Scraping page {current_page}: {paginated_url}\")\n        \n        # Send request with a delay to be respectful\n        time.sleep(random.uniform(1.0, 3.0))\n        response = requests.get(paginated_url, headers=HEADERS)\n        \n        if response.status_code != 200:\n            print(f\"Failed to retrieve page {current_page}: Status code {response.status_code}\")\n            break\n        \n        # Parse the HTML content\n        soup = BeautifulSoup(response.content, \"html.parser\")\n        \n        # Find product containers (this selector needs to be adjusted for specific sites)\n        product_containers = soup.select(\".product-item\")\n        \n        if not product_containers:\n            print(f\"No products found on page {current_page}\")\n            break\n        \n        # Extract data from each product\n        for container in product_containers:\n            try:\n                # Extract product details (adjust selectors for specific sites)\n                product_name = container.select_one(\".product-name\").text.strip()\n                product_price = container.select_one(\".product-price\").text.strip()\n                \n                # Get the relative URL and convert to absolute\n                product_url_rel = container.select_one(\"a\")[\"href\"]\n                product_url = urljoin(url, product_url_rel)\n                \n                # Try to get image URL\n                img_element = container.select_one(\"img\")\n                product_image = img_element[\"src\"] if img_element else \"No image\"\n                \n                # Add to product list\n                all_products.append({\n                    \"name\": product_name,\n                    \"price\": product_price,\n                    \"url\": product_url,\n                    \"image\": product_image\n                })\n                \n            except Exception as e:\n                print(f\"Error extracting product details: {str(e)}\")\n        \n        print(f\"Extracted {len(product_containers)} products from page {current_page}\")\n        current_page += 1\n    \n    return all_products\n\ndef save_to_csv(products, filename=\"products.csv\"):\n    \"\"\"Save the scraped products to a CSV file.\"\"\"\n    if not products:\n        print(\"No products to save\")\n        return\n    \n    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n        fieldnames = products[0].keys()\n        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n        \n        writer.writeheader()\n        for product in products:\n            writer.writerow(product)\n    \n    print(f\"Saved {len(products)} products to {filename}\")\n\ndef main():\n    # Replace with the target e-commerce URL\n    target_url = \"https://example-ecommerce.com/products\"\n    \n    # Scrape products\n    products = scrape_product_listings(target_url)\n    \n    # Save results\n    if products:\n        save_to_csv(products)\n\nif __name__ == \"__main__\":\n    main()"
    },
    {
      "id": "ml-image-classification",
      "title": "Image Classification with TensorFlow",
      "description": "Train a simple image classifier using TensorFlow and Keras",
      "language": "python",
      "icon": "üñºÔ∏è",
      "tags": ["machine learning", "TensorFlow", "image processing"],
      "code": "import tensorflow as tf\nfrom tensorflow.keras import layers, models\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n# Load and prepare the CIFAR-10 dataset\n(train_images, train_labels), (test_images, test_labels) = cifar10.load_data()\n\n# Normalize pixel values to be between 0 and 1\ntrain_images = train_images.astype('float32') / 255.0\ntest_images = test_images.astype('float32') / 255.0\n\n# One-hot encode the labels\ntrain_labels = to_categorical(train_labels, 10)\ntest_labels = to_categorical(test_labels, 10)\n\n# Define the CNN model\nmodel = models.Sequential()\nmodel.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(32, 32, 3)))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.MaxPooling2D((2, 2)))\nmodel.add(layers.Conv2D(64, (3, 3), activation='relu'))\nmodel.add(layers.Flatten())\nmodel.add(layers.Dense(64, activation='relu'))\nmodel.add(layers.Dense(10, activation='softmax'))\n\n# Compile the model\nmodel.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n# Print model summary\nmodel.summary()\n\n# Train the model\nhistory = model.fit(train_images, train_labels, epochs=10, \n                    validation_data=(test_images, test_labels))\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(test_images, test_labels)\nprint(f'Test accuracy: {test_acc:.4f}')\n\n# Plot training history\nplt.figure(figsize=(12, 4))\n\n# Plot training & validation accuracy\nplt.subplot(1, 2, 1)\nplt.plot(history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.history['val_accuracy'], label='Validation Accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.legend()\nplt.title('Training and Validation Accuracy')\n\n# Plot training & validation loss\nplt.subplot(1, 2, 2)\nplt.plot(history.history['loss'], label='Training Loss')\nplt.plot(history.history['val_loss'], label='Validation Loss')\nplt.xlabel('Epoch')\nplt.ylabel('Loss')\nplt.legend()\nplt.title('Training and Validation Loss')\n\nplt.tight_layout()\nplt.savefig('training_history.png')\nplt.show()\n\n# Save the model\nmodel.save('cifar10_classifier.h5')\nprint('Model saved to cifar10_classifier.h5')"
    }
  ]
}
